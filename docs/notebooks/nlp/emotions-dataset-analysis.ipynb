{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2f8212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T08:19:49.294037Z",
     "start_time": "2023-04-08T08:19:47.533476Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "       \n",
       "    \n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"NLP with Transformers\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "           \n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "    \n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "   \n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "    \n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "<center><img src=\"https://d4x5p7s4.rocketcdn.me/wp-content/uploads/2016/03/logo-poster-smaller.png\"/> </center>\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85541636",
   "metadata": {},
   "source": [
    "## <img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" width=\"60px\">  Pretrained Transformers\n",
    "\n",
    "In this lab, we will delve into the pretrained transformers. \n",
    "\n",
    "In particular, we will continue our running example of sentiment analysis classification problem. We will see how to use a huggingface transformer\n",
    "\n",
    "### Transformers\n",
    "\n",
    "A pre-trained transformer, such as `bert` in the hugging-face repository exists as a head-less body: in other words, they produce a latent state embeddings. The weights of the transformer has been trained one some general task, such as MLM (masked language model): mask a few tokens in a sentence, and train the transformer to reconstruct the missing tokens as accurately as possible, from a vast corpus of documents. These weights form the **checkpoint** of the transformer model. Therefore, when we download and load a transformer from the hugging-face repository, we are actually loading the model with these particular trained checkpoint weights.\n",
    "\n",
    "Such a transformer body can be used for a variety of tasks by using these latent state embeddings as input to a head meant to perform a particular task.\n",
    "\n",
    "Let us say that we would like to create a text classifier for sentiment analysis -- i.e. classify each piece of text into the sentiment it represents. Then the pipeline we can build would look like this:\n",
    "\n",
    "<img src = \"classifier-pipeline.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecc02f",
   "metadata": {},
   "source": [
    "First, we will need to tokenize the input text using a tokenizer. We covered this in a previous lab. The output of the tokenizer would be the input tokens to the transformer. The transformer will emit a hidden state embedding for each text. These embeddings become the input to the classifier network, which we can train using the gradient descent and backpropagation of gradients.\n",
    "\n",
    "Therefore, we typically would make the classifier itself a differentiable function, made of a few layers of feed forward network. Or in its most simple form, it could be single softmax layer.\n",
    "\n",
    "For the classification purposes, it is common to take only the transformer generated embedding of the `[CLS]` token as the input to the classifier, if one is using the `bert` transformer.\n",
    "\n",
    "<hr />\n",
    "<img src=\"nlp-with-transformers-book.png\"  width=\"150\" style=\"padding:20px;\" align=\"left\"> <b>Note</b>: Some of the code snippets below is inspired by, or directly taken from the book <a href=\"https://www.amazon.com/dp/1098136799\"> Natural Language Processing with Transformers, Revised Edition. </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f548f88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3665b",
   "metadata": {},
   "source": [
    "`Distilbert` is a much smaller model, that approximates the accuracy the much larger `BERT` model. Since it is much easier to play with for inferences, especially when working on laptops without gpu/tpu acceleration support, we will use it for this lab.\n",
    "\n",
    "**Note**: Considering that many of us do not have powerful laptops with tensor accelerators, we have not moved the model to the \"cuda\" device. However, if you do have that available, strongly consider using that with the syntax:\n",
    "\n",
    "```\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7b941",
   "metadata": {},
   "source": [
    "Let us explore what the output hidden state embeddings of the `distilbert` transformer looks like; first we will take a text and tokenize it. Then we will input it to the transformer, and look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4947d751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c22100c7734f5a8b209ea84b163b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6365cbf1c9fa46f7ac02b6da8384202c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bb69d19a984fb492748439bd39acfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   143, 21973, 10108, 25209, 10127,   143, 27318, 10139, 15765,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "The shape of the input is torch.Size([1, 11])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"nlptown/bert-base-multilingual-uncased-sentiment\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "text = 'A thing of beauty is a joy for ever'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(inputs)\n",
    "print(f\"The shape of the input is {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d55a9ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db52289",
   "metadata": {},
   "source": [
    "In what follows, for simplicity, we will skip the step of loading it into the gpu; however, augment the code to run it on the gpu. Hint: use `.to(device)` on the values, before passing it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc6286e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2633, -2.5239, -0.9028,  1.4483,  3.4063]]), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2633, -2.5239, -0.9028,  1.4483,  3.4063]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    \n",
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fe51f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312db6f",
   "metadata": {},
   "source": [
    "We see that the embedding for each token is a 768-dimensional vector, and there are 11 tokens for the 1 input sentence.\n",
    "\n",
    "What does the hidden state of the `[CLS]` look like? Recall that it is the first token, and we will use it's latent state to feed into the classifier. We expect a 768-dimensional vector. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b91f965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0030, 0.0023, 0.0116, 0.1216, 0.8615]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "predictions = softmax(logits, dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177d2da",
   "metadata": {},
   "source": [
    "### Load the `emotion` dataset\n",
    "\n",
    "I the previous lab, we explored the `emotion` dataset. Let's load it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bef74b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/asif/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208679fb4e76421c8ebc7bd88c629d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset('emotion')\n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078b6e4",
   "metadata": {},
   "source": [
    "Let us now define a function to act on a mini-batch  (commonly, called a batch) of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d0dbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch['text'], padding=True, truncation =True)\n",
    "    return tokens\n",
    "\n",
    "def extract_hidden_state(batch):\n",
    "    \n",
    "    inputs = {k:v for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    \n",
    "    # forward-pass the batch through the transformer,\n",
    "    # to get the hidden state of each of the tokens\n",
    "    # disable gradient computations, as we are in inference mode\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "        # return only the hidden-state of the [CLS] token\n",
    "        last_hidden_state = last_hidden_state[:, 0].numpy()\n",
    "        \n",
    "        return {'hidden_state': last_hidden_state}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d977b",
   "metadata": {},
   "source": [
    "Let us try these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65404a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(emotions['train'][:2])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0029fe",
   "metadata": {},
   "source": [
    "#### Tokenize the `emotions` training dataset\n",
    "\n",
    "With `tokenize()` function, we can now tokenize the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d45594e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/asif/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-c1a5eb64959c25f7.arrow\n",
      "Loading cached processed dataset at /home/asif/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-5f12000962fab643.arrow\n",
      "Loading cached processed dataset at /home/asif/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-27dd56d250cb57d8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "print(emotions_encoded['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa017d0",
   "metadata": {},
   "source": [
    "Our model expects inputs as `pytorch` tensors. We can ensure that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64104d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "emotions_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cee67",
   "metadata": {},
   "source": [
    "#### Forward pass through the transformer\n",
    "\n",
    "Let us now pass the entire dataset through the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa3c4a5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79bf54de838430bb373ab345a25433c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emotions_latent \u001b[38;5;241m=\u001b[39m \u001b[43memotions_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_latent_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/dataset_dict.py:851\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 851\u001b[0m     {\n\u001b[1;32m    852\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/dataset_dict.py:852\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    851\u001b[0m     {\n\u001b[0;32m--> 852\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3004\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   2998\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   2999\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3002\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3003\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3004\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3005\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3006\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3380\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3376\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3377\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3378\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3380\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3386\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3389\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3261\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3260\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3261\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3263\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3264\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3265\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[60], line 13\u001b[0m, in \u001b[0;36mto_latent_vector\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# forward-pass the batch through the transformer,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# to get the hidden state of each of the tokens\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# disable gradient computations, as we are in inference mode\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# return only the hidden-state of the [CLS] token\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     latent_vector \u001b[38;5;241m=\u001b[39m last_hidden_states[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "emotions_latent = emotions_encoded.map(to_latent_vector, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e7fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
